<!DOCTYPE html>
<html>

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@100;400&display=swap" rel="stylesheet">


<head>
  <meta charset="UTF-8">
  <title>CHARM Findings</title>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.0/papaparse.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/echarts@5.3.3/dist/echarts.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
    rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
  <!-- favicon.svg -->
  <!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸ‘</text></svg>"> -->
  <!-- <link rel="icon" href="/favicon.svg" /> -->
  <link rel="icon" href="https://images.emojiterra.com/google/noto-emoji/unicode-15.1/color/512px/1f6e0.png">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/css/bootstrap.min.css">

  <style type="text/css">
    body {
      font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      font-weight: 300;
      font-size: 20px;
      margin-left: auto;
      margin-right: auto;
    }

    @media screen and (min-width: 980px) {
      body {
        width: 980px;
      }
    }

    table {
      width: 100%;
      border-collapse: collapse;
    }

    thead th,
    tbody td {
      border: 1px solid black;
      padding: 8px;
      text-align: center;
      font-size: 17px;
      /* è°ƒæ•´å­—ä½“å¤§å°ä¸º14åƒç´  */
    }

    td strong {
      font-weight: bold;
      /* è®¾ç½®å­—ä½“ä¸ºç²—ä½“ */
    }

    thead th {
      background-color: lightgray;
    }

    tbody tr:nth-child(even) {
      background-color: #f2f2f2;
    }

    tbody tr:hover {
      background-color: #ddd;
    }

    .fixed-header {
      position: sticky;
      top: 0;
      z-index: 1;
      background-color: #f9f9f9;
    }


    h4 {
      text-align: center;
      font-weight: 300;
      line-height: 1.15em;
      font-size: 20px;
      font-weight: bold;
      /* è®¾ç½®æ ‡é¢˜çš„å­—å·å¤§å° */

    }

    h2 {
      font-size: 1.75em;
    }

    a:link,
    a:visited {
      color: #5364cc;
      text-decoration: none;
    }

    a:hover {
      color: #208799;
    }

    h3 {
      text-align: left;
    }

    h1 {
      text-align: center;
      font-size: 40px;
      font-weight: 500;
    }

    h2 {
      font-weight: 400;
      margin: 16px 0px 4px 0px;
    }

    h3 {
      font-weight: 600;
      margin: 16px 0px 4px 0px;
    }

    .paper-title {
      padding: 1px 0px 1px 0px;
    }

    h5 {
      font-size: 24px;
      color: #800080;
    }

    h6 {
      color: #008080;
    }

    ul {
      list-style-type: disc;
      margin-left: 20px;
    }

    section {
      margin: 32px 0px 32px 0px;
      text-align: justify;
      clear: both;
    }

    .col-5 {
      width: 20%;
      float: left;
    }

    .move-down {
      margin-top: 1.2cm;
    }

    .col-4 {
      width: 25%;
      float: left;
    }

    .col-3 {
      width: 33%;
      float: left;
    }

    .col-2 {
      width: 50%;
      float: left;
    }

    .col-1 {
      width: 100%;
      float: left;
    }

    .author-row,
    .affil-row {
      font-size: 17px;
    }

    .author-row-new {
      text-align: center;
    }

    .author-row-new a {
      display: inline-block;
      font-size: 17px;
      padding: 4px;
    }

    .author-row-new sup {
      color: #313436;
      font-size: 13 px;
      padding: 4px;
    }

    .affiliations-new {
      font-size: 16px;
      text-align: center;
      width: 80%;
      margin: 0 auto;
      margin-bottom: 20px;
    }

    .row {
      margin: 16px 0px 16px 0px;
    }

    .authors {
      font-size: 26px;
    }

    .affiliatons {
      font-size: 18px;
    }

    .affil-row {
      margin-top: 18px;
    }

    .teaser {
      max-width: 100%;
    }

    .text-center {
      text-align: center;
    }

    .screenshot {
      width: 256px;
      border: 1px solid #ddd;
    }

    .screenshot-el {
      margin-bottom: 16px;
    }

    hr {
      height: 1px;
      border: 0;
      border-top: 1px solid #ddd;
      margin: 0;
    }

    .material-icons {
      vertical-align: -6px;
    }

    p {
      line-height: 1.25em;
    }

    .caption {
      font-size: 20px;
      color: #000000;
      margin-top: 4px;
      margin-bottom: 10px;
      text-align: left;
      font-weight: bold;
    }


    video {
      display: block;
      margin: auto;
    }


    figure {
      display: block;
      margin: auto;
      margin-top: 10px;
      margin-bottom: 10px;
    }

    #bibtex pre {
      font-size: 14px;
      background-color: #eee;
      padding: 16px;
    }

    .blue {
      color: #2c82c9;
      font-weight: bold;
    }

    .orange {
      color: #d35400;
      font-weight: bold;
    }

    .flex-row {
      display: flex;
      flex-flow: row wrap;
      padding: 0;
      margin: 0;
      list-style: none;
    }

    .flex-row-center {
      display: flex;
      flex-flow: row wrap;
      padding: 0;
      margin: 0;
      list-style: none;
      justify-content: center;
      text-align: center;
    }

    .flex-container {
      display: flex;
      flex-wrap: wrap;
    }

    .flex-item {
      flex: 0 0 50%;
      padding: 10px;
      box-sizing: border-box;
    }

    .paper-btn-coming-soon {
      position: relative;
      top: 0;
      left: 0;
    }

    .coming-soon {
      position: absolute;
      top: -15px;
      right: -15px;
    }

    .center {
      margin-left: 10.0%;
      margin-right: 10.0%;
    }

    .paper-btn {
      position: relative;
      text-align: center;

      display: inline-block;
      margin: 8px;
      padding: 8px 8px;

      border-width: 0;
      outline: none;
      border-radius: 2px;

      background-color: #E0F7FA;
      color: #01579B !important;
      font-size: 20px;
      width: 200px;
      font-weight: 600;
    }

    .paper-btn-tapestry {
      position: relative;
      text-align: center;

      display: inline-block;
      margin: 8px;
      padding: 8px 8px;

      border-width: 0;
      outline: none;
      border-radius: 2px;

      background-color: #5364cc;
      color: white !important;
      font-size: 20px;
      width: 200px;
      font-weight: 600;
    }

    .paper-btn-parent {
      display: flex;
      justify-content: center;
      margin: 16px 0px;
    }

    .paper-btn:hover {
      opacity: 0.85;
    }

    .container {
      margin-left: auto;
      margin-right: auto;
      padding-left: 16px;
      padding-right: 16px;
    }

    .venue {
      font-size: 23px;
    }

    .topnav {
      background-color: #EEEEEE;
      overflow: hidden;
    }

    .topnav div {
      max-width: 1070px;
      margin: 0 auto;
    }

    .topnav a {
      display: inline-block;
      color: black;
      text-align: center;
      vertical-align: middle;
      padding: 16px 16px;
      text-decoration: none;
      font-size: 18px;
    }

    .topnav img {
      padding: 2px 0px;
      width: 100%;
      margin: 0.2em 0px 0.3em 0px;
      vertical-align: middle;
    }

    pre {
      font-size: 0.9em;
      padding-left: 7px;
      padding-right: 7px;
      padding-top: 3px;
      padding-bottom: 3px;
      border-radius: 3px;
      background-color: rgb(235, 235, 235);
      overflow-x: auto;
    }

    .download-thumb {
      display: flex;
    }

    @media only screen and (max-width: 620px) {
      .download-thumb {
        display: none;
      }
    }

    .paper-stuff {
      width: 50%;
      font-size: 20px;
    }

    @media only screen and (max-width: 620px) {
      .paper-stuff {
        width: 100%;
      }
    }

    * {
      box-sizing: border-box;
    }

    .column {
      text-align: center;
      float: left;
      width: 16.666%;
      padding: 5px;
    }

    .column3 {
      text-align: center;
      float: left;
      width: 33.333%;
      padding: 5px;
    }

    .column4 {
      text-align: center;
      float: left;
      width: 50%;
      padding: 5px;
    }

    .column5 {
      text-align: center;
      float: left;
      width: 20%;
      padding: 5px;
    }

    .column10 {
      text-align: center;
      float: left;
      width: 10%;
      padding: 5px;
    }

    .border-right {
      border-right: 1px solid black;
    }

    .border-bottom {
      border-bottom: 1px solid black;
    }


    .row-center {
      margin: 16px 0px 16px 0px;
      text-align: center;
    }

    /* Clearfix (clear floats) */
    .row::after {
      content: "";
      clear: both;
      display: table;
    }

    .img-fluid {
      max-width: 100%;
      height: auto;
    }

    .figure-img {
      margin-bottom: 0.5rem;
      line-height: 1;
    }

    .rounded-circle {
      border-radius: 50% !important;
    }

    /* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
    @media screen and (max-width: 500px) {
      .column {
        width: 100%;
      }
    }

    @media screen and (max-width: 500px) {
      .column3 {
        width: 100%;
      }
    }

    .paper-btn-parent2 {
      display: flex;
      justify-content: center;
      align-items: center;
    }

    .paper-btn-child {
      margin: 0 10px;
      /* æ·»åŠ é€‚å½“çš„é—´è· */
    }

    .separator {
      width: 5px;
      /* è®¾ç½®åˆ†éš”ç¬¦çš„å®½åº¦ */
    }
  </style>
</head>

<body>
  <div class="container">
    <div id="content" class="container-fluid d-flex flex-column align-items-center gap-3">
      <h1 class="text-nowrap mt-5">âœ¨ CHARM Findings âœ¨</h1>
      <h3 class="fw-light text-nowrap"><small id="warning">Benchmarking Chinese Commonsense Reasoning of LLMs: From
          Chinese-Specifics to Reasoning-Memorization Correlations.<br></small></h3>

      <p></p>
      <div class="paper-btn-parent2">
        <a class="paper-btn-child" href="./findings.html">English</a>
        <span class="separator">|</span>
        <a class="paper-btn-child" href="./findings_ZH.html">ä¸­æ–‡</a>
      </div>

      <div style="clear: both">
        <div class="paper-btn-parent">
          <a class="paper-btn" href="https://arxiv.org/abs/2403.14112">
            <span class="material-icons"> description </span>
            Paper
          </a>
          <a class="paper-btn" href="https://github.com/opendatalab/CHARM">
            <span class="material-icons"> code </span>
            Code
          </a>
          <a class="paper-btn" href="./index.html">
            <span class="material-icons"> description </span>
            Project Page
          </a>
          <a class="paper-btn" href="./leaderboard.html">
            <span class="material-icons"> description </span>
            Leaderboard
          </a>
        </div>
      </div>
    </div>


    <p><strong>ğŸ’¡ This page will showcase some conclusions and findings based on CHARM, including Prompt Strategy,
        Integrated
        Reasoning vs Memorization, and Memorization-Independent Reasoning.</strong></p>
    <div>
      <section id="prompt">
        <h3>Prompt Strategy</h3>
        <p></p>

        <body>
          <p>We selected 5 commonly used prompt strategies, and assessed the performance of the 19 LLMs on CHARM
            reasoning task. We evaluated the currently commonly used LLMs, which can be divided into two categories:</p>
          (1) 7 English LLMs, including GPT-3.5, GPT4, LLaMA-2 (7B,13B,70B), and Vicuna (7B,13B).<br>
          (2) 12 Chinese-oriented LLMs, including
          ChatGLM3 (6B), Baichuan2 (7B,13B), InternLM2 (7B,20B), Yi (6B,34B), DeepSeek (7B,67B) and Qwen (7B,14B,72B).
        </body>
        <p></p>
        <h4> Examples of prompt strategies </h4>
        <table align="center">
          <thead class="fixed-header">
            <tr style="text-align: center;">
              <th>Prompt Strategy</th>
              <th>Description</th>
              <th>Example</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Direct</td>
              <td>The LLM does not perform intermediate reasoning and directly predicts the answer.</td>
              <td>Qï¼šä»¥ä¸‹é™ˆè¿°æ˜¯å¦åŒ…å«æ—¶ä»£é”™è¯¯ï¼Œè¯·é€‰æ‹©æ­£ç¡®é€‰é¡¹ã€‚ä¸€ä¸ªæ¥å—äº†ä¹‰åŠ¡æ•™è‚²ã€å…·å¤‡åŸºæœ¬å¸¸è¯†çš„äººä¼šå¦‚ä½•é€‰æ‹©ï¼Ÿæç™½ç”¨é’¢ç¬”å†™è¯—ã€‚<br>é€‰é¡¹ï¼š(A) æ˜¯ (B) å¦ <br>A:(A)</td>
            </tr>
            <tr>
              <td>ZH-CoT</td>
              <td>The LLM conducts intermediate reasoning in Chinese before producing the answer.</td>
              <td>Qï¼šä»¥ä¸‹é™ˆè¿°æ˜¯å¦åŒ…å«æ—¶ä»£é”™è¯¯ï¼Œè¯·é€‰æ‹©æ­£ç¡®é€‰é¡¹ã€‚ä¸€ä¸ªæ¥å—äº†ä¹‰åŠ¡æ•™è‚²ã€å…·å¤‡åŸºæœ¬å¸¸è¯†çš„äººä¼šå¦‚ä½•é€‰æ‹©ï¼Ÿæç™½ç”¨é’¢ç¬”å†™è¯—ã€‚<br>é€‰é¡¹ï¼š(A) æ˜¯ (B)
                å¦<br>Aï¼šè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥æ€è€ƒã€‚è¿™ä¸ªé™ˆè¿°æåˆ°äº†â€œæç™½â€ï¼Œä»–æ˜¯ä¸­å›½å”æœæ—¶æœŸçš„è¯—äººã€‚è€Œé™ˆè¿°ä¸­æåˆ°çš„â€œé’¢ç¬”â€æ˜¯ç°ä»£è®¾å¤‡ï¼Œå› æ­¤æç™½ä¸å¯èƒ½ä½¿ç”¨é’¢ç¬”å†™è¯—ï¼Œè¯¥é™ˆè¿°åŒ…å«æ—¶ä»£é”™è¯¯ã€‚æ‰€ä»¥ç­”æ¡ˆæ˜¯(A)ã€‚</td>
            </tr>
            <tr>
              <td>EN-CoT</td>
              <td>The reasoning process of CoT is in English for the Chinese questions.</td>
              <td>Qï¼šä»¥ä¸‹é™ˆè¿°æ˜¯å¦åŒ…å«æ—¶ä»£é”™è¯¯ï¼Œè¯·é€‰æ‹©æ­£ç¡®é€‰é¡¹ã€‚ä¸€ä¸ªæ¥å—äº†ä¹‰åŠ¡æ•™è‚²ã€å…·å¤‡åŸºæœ¬å¸¸è¯†çš„äººä¼šå¦‚ä½•é€‰æ‹©ï¼Ÿæç™½ç”¨é’¢ç¬”å†™è¯—ã€‚<br>é€‰é¡¹ï¼š(A) æ˜¯ (B) å¦<br>Aï¼šLet's think step by
                step.This
                statement mentions "Li Bai", a poet from the Tang Dynasty in China. The "pen" mentioned in the statement
                is a modern device, so it is impossible for Li Bai to write poetry with a pen. This statement contains
                errors from the times. So the answer is (A).</td>
            </tr>
            <tr>
              <td>Translate-EN</td>
              <td>We used the <a href="https://www.deepl.com/translator">DeepL
                  api</a> to translate our benchmark into English, and then used English CoT for reasoning.
              </td>
              <td>Q: Choose the correct option if the following statement contains an anachronism. How would a person
                with compulsory education and basic common sense choose?Li Bai wrote poetry with a fountain
                pen.<br>Options:(A) Yes (B) No<br>A: Let's think step by step.The statement mentions "Li Bai", a Chinese
                poet
                from the Tang Dynasty. The "fountain pen" mentioned in the statement is a modern device, so Li Bai could
                not have used a fountain pen to write his poems, and the statement contains an anachronism. The answer
                is (A).</td>
            </tr>
            <tr>
              <td>XLT</td>
              <td>The template prompt XLT was used to change the original question into an English
                request, solve it step by step, and finally format the
                answer for output.</td>
              <td>I want you to act as a commonsense reasoning expert for
                Chinese.Requestï¼šä»¥ä¸‹é™ˆè¿°æ˜¯å¦åŒ…å«æ—¶ä»£é”™è¯¯ï¼Œè¯·é€‰æ‹©æ­£ç¡®é€‰é¡¹ã€‚ä¸€ä¸ªæ¥å—äº†ä¹‰åŠ¡æ•™è‚²ã€å…·å¤‡åŸºæœ¬å¸¸è¯†çš„äººä¼šå¦‚ä½•é€‰æ‹©ï¼Ÿæç™½ç”¨é’¢ç¬”å†™è¯—ã€‚é€‰é¡¹ï¼š(A) æ˜¯ (B) å¦<br>You should retell
                the request in English.<br>You should do the answer step by step to choose the right answer.You should
                step-by-step answer the request.<br>You should tell me the answer in this format 'So the answer
                is'.<br>Request: How would a typical person answer each of the following statements whether it contains
                an
                anachronism? Li Bai writes poetry with a pen. Option:(A) Yes (B) No<br>Step-by-step answer:<br>1.This
                statement mentions "Li Bai", a poet from the Tang Dynasty in China.<br>2.The pen mentioned in the
                statement is a modern device.<br>3. so, it is impossible for Li Bai to write poetry with a pen. This
                statement contains errors from the times. <br>So the answer is (A).</td>
            </tr>
          </tbody>
        </table>
        <p></p>
        <p>
          We tested the combinations of the 19 LLMs and the 5 prompt strategies in CHARM reasoning tasks. The results
          are shown in the table below.
        </p>
        <p></p>
        <h4> Averaged accuracy on CHARM reasoning tasks</h4>
        <table align="center">
          <thead class="fixed-header">
            <tr>
              <th></th>
              <th><strong>Prompt</strong></th>
              <th><strong>Avg. all LLMs</strong></th>
              <th><strong>Avg. CN-LLMs</strong></th>
              <th><strong>Avg. EN-LLMs</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="5">Avg. all domains</td>
              <td>Direct</td>
              <td>46.28</td>
              <td>48.41</td>
              <td>42.64</td>
            </tr>
            <tr>
              <td>ZH-CoT</td>
              <td>56.66</td>
              <td><strong>62.40</strong></td>
              <td>46.81</td>
            </tr>
            <tr>
              <td>EN-CoT</td>
              <td>54.46</td>
              <td>58.19</td>
              <td>48.06</td>
            </tr>
            <tr>
              <td>Translate-EN</td>
              <td>53.88</td>
              <td>55.51</td>
              <td>51.07</td>
            </tr>
            <tr>
              <td>XLT</td>
              <td>56.81</td>
              <td>59.09</td>
              <td><strong>52.90</strong></td>
            </tr>
            <tr>
              <td rowspan="5">Avg. Chinese domain</td>
              <td>Direct</td>
              <td>45.43</td>
              <td>47.76</td>
              <td>41.44</td>
            </tr>
            <tr>
              <td>ZH-CoT</td>
              <td><strong>56.35</strong></td>
              <td><strong>62.23</strong></td>
              <td>46.26</td>
            </tr>
            <tr>
              <td>EN-CoT</td>
              <td>52.06</td>
              <td>56.36</td>
              <td>44.68</td>
            </tr>
            <tr>
              <td>Translate-EN</td>
              <td>47.25</td>
              <td>47.82</td>
              <td>46.27</td>
            </tr>
            <tr>
              <td>XLT</td>
              <td>53.80</td>
              <td>56.63</td>
              <td><strong>48.96</strong></td>
            </tr>
            <tr>
              <td rowspan="5">Avg. global domain</td>
              <td>Direct</td>
              <td>47.13</td>
              <td>49.05</td>
              <td>43.85</td>
            </tr>
            <tr>
              <td>ZH-CoT</td>
              <td>56.96</td>
              <td>62.57</td>
              <td>47.35</td>
            </tr>
            <tr>
              <td>EN-CoT</td>
              <td>56.85</td>
              <td>60.01</td>
              <td>51.44</td>
            </tr>
            <tr>
              <td>Translate-EN</td>
              <td><strong>60.50</strong></td>
              <td><strong>63.20</strong></td>
              <td>55.87</td>
            </tr>
            <tr>
              <td> XLT</td>
              <td>59.82</td>
              <td>61.56</td>
              <td><strong>56.84</strong></td>
            </tr>
        </table>

        <p></p>

        <body>
          <p>ğŸ’¡ Results showed that LLMs' orientation and the task's domain affect prompt strategy performance, which
            enriches previous research findings.</p>
          <p><strong>From the LLM dimension,</strong> it's clear that various LLMs prefer different prompt strategies:
            XLT
            consistently excels for English LLMs among the 5 strategies, while for Chinese-oriented LLMs, despite some
            complexity, ZH-CoT generally performs best.</p>

          <p><strong>From the commonsense domain dimension,</strong> strategies that use English for reasoning (like
            XLT,
            Translate-EN, etc.) are suitable for the global domain; however, ZH-CoT generally performs better in the
            Chinese domain.</p>

          <p>The conclusion here differs from previous studies (<a href="https://arxiv.org/pdf/2305.07004.pdf">Huang et
              al., 2023a</a>, <a href="https://aclanthology.org/2023.emnlp-main.491.pdf">Zhang et al., 2023a</a>, <a
              href="https://arxiv.org/pdf/2210.03057.pdf">Shi et al., 2022</a>), which suggested that employing
            English for non-English reasoning tasks was more effective than using the native language.</p>


        </body>

      </section>

      <section id="teaser-image">

        <body>
          <h3>Integrated Reasoning vs Memorization</h3>
          <p> We evaluated the correlation between integrated reasoning and memorization on the MRI tasks. Here is
            the
            average performance of the LLMs on the 4 MRI tasks.</p>
        </body>
        <hr>
        <center>
          <figure>
            <a>
              <img width="80%" src="./figure/Fig_mem-integrated_rea-avg_v4.png">
            </a>
            <p class="caption" , style="text-align: center;">
              Averaged accuracy across the 4 MRI tasks in
              the Chinese commonsense domain.
            </p>
          </figure>
        </center>
        <p>As shown in the figure, the 19 LLMs can be roughly divided into three types:</p>
        <ul>
          <li><strong>Type I: Low memorization and low integrated reasoning ability.</strong> We found that apart from
            OpenAI's GPT series, all other English LLMs belong to this type.</li>
          <li><strong>Type II: High memorization and medium integrated reasoning ability.</strong> GPT3.5 and all
            Chinese-oriented LLMs below 30B belong to this type. Some LLMs have high memorization performance but
            relatively poor integrated reasoning ability.</li>
          <li><strong>Type III: Ultra-high memorization and high integrated reasoning ability.</strong> This category
            includes GPT4 and the three Chinese-oriented LLMs exceeding a size of 30B.</li>
        </ul>

        <h3>Memorization-Independent Reasoning</h3>
        <p>We propose two methods to compare the LLMs' memorization-independent reasoning on the MRI tasks:
          <strong>Mono-LLM-Memorization (FRMM) and Memorization-Independent Battles among LLMs (MIB)</strong>. For
          specific details about the FRMM and MIB methods, please refer to the <a
            href="https://arxiv.org/abs/2403.14112">paper</a>. Here are the results of the FRMM
          and MIB methods.
        </p>
        <table align="center">
          <h4>Leaderboard on the MRI tasks.</h4>
          <thead class="fixed-header">
            <tr>
              <th rowspan="2">Rank</th>
              <th rowspan="2">Integrated Reasoning</th>
              <th colspan="2">Memorization-independent Reasoning</th>
            </tr>
            <tr>
              <th>FRMM</th>
              <th>MIB</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>DeepSeek-67B</td>
              <td>Yi-34B (<span style="color:red;">â†‘3</span>)</td>
              <td>GPT-4 (<span style="color:red;">â†‘2</span>)</td>
            </tr>
            <tr>
              <td>2</td>
              <td>Qwen-72B</td>
              <td>DeepSeek-67B (<span style="color:green;">â†“1</span>)</td>
              <td>Yi-34B (<span style="color:red;">â†‘2</span>)</td>
            </tr>
            <tr>
              <td>3</td>
              <td>GPT-4</td>
              <td>GPT-4 (-)</td>
              <td>Qwen-72B (<span style="color:green;">â†“1</span>)</td>
            </tr>
            <tr>
              <td>4</td>
              <td>Yi-34B</td>
              <td>Qwen-72B (<span style="color:green;">â†“2</span>)</td>
              <td>DeepSeek-67B (<span style="color:green;">â†“3</span>)</td>
            </tr>
            <tr>
              <td>5</td>
              <td>Qwen-14B</td>
              <td>GPT-3.5 (<span style="color:red;">â†‘2</span>)</td>
              <td>GPT-3.5 (<span style="color:red;">â†‘2</span>)</td>
            </tr>
            <tr>
              <td>6</td>
              <td>InternLM2-20B</td>
              <td>Qwen-14B (<span style="color:green;">â†“1</span>)</td>
              <td>Qwen-14B (<span style="color:green;">â†“1</span>)</td>
            </tr>
            <tr>
              <td>7</td>
              <td>GPT-3.5</td>
              <td>InternLM2-20B (<span style="color:green;">â†“1</span>)</td>
              <td>InternLM2-20B (<span style="color:green;">â†“1</span>)</td>
            </tr>
            <tr>
              <td>8</td>
              <td>InternLM2-7B</td>
              <td>InternLM2-7B (-)</td>
              <td>InternLM2-7B (-)</td>
            </tr>
            <tr>
              <td>9</td>
              <td>DeepSeek-7B</td>
              <td>Baichuan2-13B (<span style="color:red;">â†‘1</span>)</td>
              <td>Baichuan2-13B (<span style="color:red;">â†‘1</span>)</td>
            </tr>
            <tr>
              <td>10</td>
              <td>Baichuan2-13B</td>
              <td>DeepSeek-7B (<span style="color:green;">â†“1</span>)</td>
              <td>DeepSeek-7B (<span style="color:green;">â†“1</span>)</td>
            </tr>
            <tr>
              <td>11</td>
              <td>Baichuan2-7B</td>
              <td>Yi-6B (<span style="color:red;">â†‘3</span>)</td>
              <td>Baichuan2-7B (-)</td>
            </tr>
            <tr>
              <td>12</td>
              <td>ChatGLM3-6B</td>
              <td>ChatGLM3-6B (-)</td>
              <td>ChatGLM3-6B (-)</td>
            </tr>
            <tr>
              <td>13</td>
              <td>Qwen-7B</td>
              <td>Baichuan2-7B (<span style="color:green;">â†“2</span>)</td>
              <td>Qwen-7B (-)</td>
            </tr>
            <tr>
              <td>14</td>
              <td>Yi-6B</td>
              <td>Qwen-7B (<span style="color:green;">â†“1</span>)</td>
              <td>Yi-6B (-)</td>
            </tr>
            <tr>
              <td>15</td>
              <td>LLaMA-2-70B</td>
              <td>LLaMA-2-13B (<span style="color:red;">â†‘1</span>)</td>
              <td>LLaMA-2-13B (<span style="color:red;">â†‘1</span>)</td>
            </tr>
            <tr>
              <td>16</td>
              <td>LLaMA-2-13B</td>
              <td>LLaMA-2-70B (<span style="color:green;">â†“1</span>)</td>
              <td>LLaMA-2-70B (<span style="color:green;">â†“1</span>)</td>
            </tr>
            <tr>
              <td>17</td>
              <td>Vicuna-13B-v1.5</td>
              <td>Vicuna-13B-v1.5 (-)</td>
              <td>Vicuna-13B-v1.5 (-)</td>
            </tr>
            <tr>
              <td>18</td>
              <td>Vicuna-7B-v1.5</td>
              <td>LLaMA-2-7B (<span style="color:red;">â†‘1</span>)</td>
              <td>Vicuna-7B-v1.5 (-)</td>
            </tr>
            <tr>
              <td>19</td>
              <td>LLaMA-2-7B</td>
              <td>Vicuna-7B-v1.5 (<span style="color:green;">â†“1</span>)</td>
              <td>LLaMA-2-7B (-)</td>
            </tr>
          </tbody>
        </table>

      </section>
      <section>

        <body>
          <p>ğŸ’¡ If Language Models (LLMs) provide incorrect responses to retained reasoning questions, these can be
            termed
            as
            memorization-independent reasoning errors. We analyzed these errors by manually reviewing the reasoning
            process and categorized them into four main types:</p>
          <ul>
            <li><strong>Understanding Error:</strong> In this case, the LLM was unable to accurately comprehend the
              question, including misunderstanding the content, ignoring or even modifying important information in
              the premise, and failing to grasp the core query of the question.</li>
            <li><strong>Knowledge Error:</strong> The LLM incorporated inaccurate knowledge during the reasoning
              process. It's important to highlight that the knowledge pieces related to the reasoning question were
              previously examined in the related memorization questions, which the LLM answered correctly. However,
              the LLM output incorrect information during the reasoning phase.</li>
            <li><strong>Logical Error:</strong> The LLM made logical reasoning errors, such as mathematical reasoning
              errors, inability to reach the correct conclusion based on sufficient information, or reaching the
              correct conclusion but outputting the wrong option.</li>
            <li><strong>Other Errors:</strong> These are other scattered, relatively rare types of errors.</li>
          </ul>

          <p>ğŸ’¡ Distribution of the Memorization-Independent Reasoning Errors</p>
          <center>
            <figure>
              <a>
                <img width="95%" src="./figure/Fig_pie_chart-mem_independent_err-3LLms.png">
              </a>
              <p class="caption" , style="text-align: center;">
                Istribution of the memorization-independent
                reasoning errors
              </p>
            </figure>
          </center>
          <p>ğŸ’¡ Examples of the 3 types of memorization-independent reasoning errors of LLMs</p>
          <center>
            <figure>
              <a>
                <img width="95%" src="./figure/Fig_mem-independent-rea-err-example.png">
              </a>
              <p class="caption" , style="text-align: center;">
                Examples of the 3 types of memorization-independent reasoning errors of LLMs.

              </p>
            </figure>
          </center>
        </body>
        <h3>ğŸ–Šï¸ Citation</h3>
        <pre>
    @misc{sun2024benchmarking,
          title={Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations}, 
          author={Jiaxing Sun and Weiquan Huang and Jiang Wu and Chenya Gu and Wei Li and Songyang Zhang and Hang Yan and Conghui He},
          year={2024},
          eprint={2403.14112},
          archivePrefix={arXiv},
          primaryClass={cs.CL}
    }
    </pre>


    </div>


  </div>
</body>

</html>